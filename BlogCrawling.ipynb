{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlogCrawling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2T-RNhMlhLK"
      },
      "source": [
        "### 모듈 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ73KV7EldJo"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "from urllib.parse import quote\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3goE4UIzlwB2"
      },
      "source": [
        "def get_posts(query, page_num):\n",
        "    url_query = quote(query)  # 검색어 형태로 변경해주는 함수\n",
        "\n",
        "    #url\n",
        "    url = 'https://search.naver.com/search.naver?query=' + url_query + '&nso=&where=blog&sm=tab_opt'\n",
        "\n",
        "    #data 저장\n",
        "    blog_df = pd.DataFrame(columns=('Title', 'Date', 'Blog Url', 'Post'))\n",
        "    idx = 0\n",
        "    # 스크롤을 위한 driver선언\n",
        "    driver = webdriver.Chrome()\n",
        "    driver.get(url=url)\n",
        "\n",
        "    # 입력한 페이지 수 만큼 스크롤\n",
        "    for i in range(page_num):\n",
        "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
        "        time.sleep(1)\n",
        "        # print(i)\n",
        "\n",
        "    # 크롤링 과정 : 검색화면 - 글 제목, 작성 날짜(중요x라서 형식에 맞게 변환x),url 스크래핑\n",
        "    # url-블로그 포스터 전문 확인 가능\n",
        "    # 크롤링 방지용 iframe 속으로 들어가서, post의 text만 크롤링하여 저장\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    posts = soup.find_all('div', {'class': 'total_wrap api_ani_send'})  # 검색화면의 블로그 목록\n",
        "\n",
        "    for post in posts:\n",
        "        # post 제목, 날짜\n",
        "        title = post.select_one('.api_txt_lines.total_tit').text\n",
        "        date = post.find('span', {'class': 'sub_time sub_txt'}).get_text()\n",
        "\n",
        "        # url->블로그 창 연결\n",
        "        post_url = post.find('a', {'class': 'api_txt_lines total_tit'}).get('href')\n",
        "\n",
        "        try:\n",
        "            post_link = urllib.request.urlopen(post_url).read()\n",
        "            # post_link = req.urlopen(post_url).read()\n",
        "            post_html = BeautifulSoup(post_link, 'html.parser')\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # iframe src를 통해 원본 접근\n",
        "        for main_frame in post_html.select(\"iframe#mainFrame\"):\n",
        "            frame_url = \"https://blog.naver.com\" + main_frame.get('src')\n",
        "            post_text = urllib.request.urlopen(frame_url).read()\n",
        "            post_html = BeautifulSoup(post_text, 'html.parser')\n",
        "            post_content_text = \"\"\n",
        "\n",
        "            for post_content in post_html.find_all('div', {'class': 'se-main-container'}):\n",
        "                post_content_text = post_content.get_text()\n",
        "\n",
        "            # 형식이 다른 블로그 존재, 예외처리\n",
        "            if post_content_text == \"\":\n",
        "                for post_content in post_html.find_all('div', {'id': 'postViewArea'}):\n",
        "                    post_content_text = post_content.get_text()\n",
        "            post_content_text = post_content_text.replace('\\n', '')\n",
        "        blog_df.loc[idx] = [title, date, post_url, post_content_text]\n",
        "        idx += 1\n",
        "    return blog_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yphTKexlyum"
      },
      "source": [
        "query = input(\"검색: \")\n",
        "page_num = int(input(\"page 수: \"))  # 한 페이지당 약 30개\n",
        "blog_df= get_posts(query, page_num)\n",
        "print(blog_df.head())\n",
        "blog_df.to_csv('blog_data_info.csv',encoding= 'utf-8-sig')\n",
        "print(\"끝\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}